<!doctype html><html lang=en-US><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>Small Bytes</title><meta content=article property=og:type><meta content="Small Bytes" property=og:site_name><meta content="Small Bytes" property=og:title><meta content="Occasional posts about what I've been learning and doing" itemprop=about name=description><link href=https://d-j-harris.github.io/assets/icon.png rel=icon type=image/png><link href=https://d-j-harris.github.io/main.css rel=stylesheet><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css integrity=sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+ rel=stylesheet><script crossorigin defer integrity=sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js></script><script crossorigin defer integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk onload=renderMathInElement(document.body); src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js></script><body><header><section class=nav-container><a class=nav-title href=https://d-j-harris.github.io/> <img alt=Logo src=/assets/icon.png style=width:auto;height:3em> <h2>Small Bytes</h2> </a><nav><ul><li><a href=https://d-j-harris.github.io>Home</a><li><a href=https://d-j-harris.github.io/tags>Tags</a></ul></nav><div class=darkmode><input class=toggle id=darkmode-toggle tabindex=-1 type=checkbox><label for=darkmode-toggle id=toggle-label-light tabindex=-1><svg style="enable-background:new 0 0 35 35" viewbox="0 0 35 35" id=dayIcon version=1.1 x=0px xml:space=preserve xmlns=http://www.w3.org/2000/svg xmlns:xlink=http://www.w3.org/1999/xlink y=0px><title>Change to light mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5 S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5 C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6 C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9 c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44 l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5 c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06 L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2 C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29 c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7 C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5 c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"/></svg></label><label for=darkmode-toggle id=toggle-label-dark tabindex=-1><svg style="enable-background='new 0 0 100 100'" viewbox="0 0 100 100" id=nightIcon version=1.1 x=0px xml:space=preserve xmlns=http://www.w3.org/2000/svg xmlns:xlink=http://www.w3.org/1999/xlink y=0px><title>Change to dark mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571 C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23 c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369 c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65 c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"/></svg></label></div><script>const userPref=window.matchMedia(`(prefers-color-scheme: light)`).matches?`light`:`dark`;const currentTheme=localStorage.getItem(`theme`)??userPref;const syntaxTheme=document.querySelector(`#theme-link`);if(currentTheme){document.documentElement.setAttribute(`saved-theme`,currentTheme)};const switchTheme=a=>{let b=`saved-theme`,d=`theme`,e=`light`,c=`dark`;if(a.target.checked){document.documentElement.setAttribute(b,c);localStorage.setItem(d,c)}else{document.documentElement.setAttribute(b,e);localStorage.setItem(d,e)}};window.addEventListener(`DOMContentLoaded`,()=>{const a=document.querySelector(`#darkmode-toggle`);a.addEventListener(`change`,switchTheme,!1);if(currentTheme===`dark`){a.checked=!0}})</script></section></header><main><article class=post><section class=post-info><span>Written by</span> Daniel Harris<br><span>on </span><time datetime=2024-10-24>October 24, 2024</time></section><h1 class=post-title>Building a Lox Interpreter in Safe Rust</h1><section class=post-line></section><ul><li><a href=https://d-j-harris.github.io/crafting-interpreters/#background>Background</a><li><a href=https://d-j-harris.github.io/crafting-interpreters/#garbage-collection>Garbage Collection</a> <ul><li><a href=https://d-j-harris.github.io/crafting-interpreters/#issues-in-rust>Issues in Rust</a><li><a href=https://d-j-harris.github.io/crafting-interpreters/#can-it-be-done-safetm>Can It Be Done safe™?</a></ul><li><a href=https://d-j-harris.github.io/crafting-interpreters/#performance-optimisation>Performance Optimisation</a> <ul><li><a href=https://d-j-harris.github.io/crafting-interpreters/#profiling-tips-in-rust>Profiling Tips in Rust</a><li><a href=https://d-j-harris.github.io/crafting-interpreters/#techniques>Techniques</a><li><a href=https://d-j-harris.github.io/crafting-interpreters/#results>Results</a></ul></ul><hr><blockquote><p>Code for this project can be found <a href=https://github.com/D-J-Harris/chef/tree/lox>here</a></blockquote><br><p>I recently decided to pick up the famous (among SWEs at least) <a href=https://craftinginterpreters.com/>Crafting Interpreters</a> book, and try my hand at writing its <a href=https://craftinginterpreters.com/a-bytecode-virtual-machine.html>Part III bytecode interpreter</a> in safe Rust. This code is written in C, so it would be an interesting exercise in figuring out the differences between the two languages and any relative strengths or limitaitons.<p>This is by no means an original work, and throughout the development process I had the pleasure of learning from several other Rust implementations that exist in the wild. In particular, I would like to credit<ul><li><a href=https://github.com/ceronman/loxido/tree/unsafe/src>Loxido</a> and the insightful accompanying <a href=https://ceronman.com/2021/07/22/my-experience-crafting-an-interpreter-with-rust/>blog</a><li><a href=https://github.com/ajeetdsouza/loxcraft/tree/main>Loxcraft</a>, which takes performance to the next level<li><a href=https://github.com/Folyd/lox-lang>Lox-lang</a> which I discovered later on and follows the same ideas for garbage collection</ul><h1 id=background>Background</h1><p>The objective of the book is to write an interpreter from the Lox language (grammar <a href=https://craftinginterpreters.com/appendix-i.html>here</a>) from scratch, which brings together multiple components; scanning and parsing of some source code:<pre style=color:#dcdcdc;background-color:#1e1e1e><code><span>var message = "Hello, World!";
</span><span>print message;
</span></code></pre><p>Compilation of that parsing output into some intermediate representation (in the form of bytecode):<pre style=color:#dcdcdc;background-color:#1e1e1e><code><span>Opcode    Line
</span><span>0000         1  Constant       [constant: Hello, World!]
</span><span>0002         |  DefineGlobal   [constant: message]
</span><span>0004         2  GetGlobal      [constant: message]
</span><span>0006         |  Print
</span><span>0007         |  Nil
</span><span>0008         |  Return
</span></code></pre><p>and finally interpretation of that bytecode into the final output: <code>Hello, World!</code><p>This blog does not set out to re-teach the book’s content, but I think one area in particular is worth diving into a bit more depth - garbage collection. Lox is a high level language, and its memory is handled behind-the-scenes by the implementing language.<h1 id=garbage-collection>Garbage Collection</h1><p>Immediately, this was one clear difference between C and Rust. The original implementation uses the mark and sweep alogorithm, which is a form of <a href=https://en.wikipedia.org/wiki/Tracing_garbage_collection>tracing garbage collection</a> that involves maintaining shared pointers to objects allocated on the heap, and scanning through them from some root location to find which objects are reachable - and therefore conversely which objects are unreachable and up for de-allocation.<p>This seemed like a perfect opportunity to deviate from the book, and flex some Rust muscles. Under the hood, Rust doesn’t use a “traditional” garbage collector, but instead implements resource management through <a href=https://doc.rust-lang.org/rust-by-example/scope/raii.html>RAII</a> (Resource Acquisition Is Initialization) and reference counting. So instead of manually implementing a garbage collector, I could let Rust do it for me, right?<h2 id=issues-in-rust>Issues in Rust</h2><p>First we must address shared borrows of the heap-allocated objects. At runtime there will be at least two places we want to hold references to our objects, and likey more:<ol><li>On the stack; Lox is a stack-based language, and we will be juggling references to values on and off the stack as the progamme runs.<li>In our compiled code chunks; we allocate at runtime and hold references until they are no longer needed:</ol><pre class=language-rust data-lang=rust style=color:#dcdcdc;background-color:#1e1e1e><code class=language-rust data-lang=rust><span style=color:#569cd6>pub struct </span><span>CodeBlock {
</span><span>    </span><span style=color:#569cd6>pub </span><span>constants: [Value; CONSTANTS_MAX_COUNT],
</span><span>    ...
</span><span>}
</span></code></pre><p>This <code>Value</code> item is an enum, whose variants hold data on the stack - either primitive types, or references to the heap. Since we need shared borrowing (and in a single-threaded environment) these references take the form of reference counted type <code>Rc&LTT></code>. Reference counts increase as more owners share the data, and as owners drop out of scope eventually the reference count ticks down to zero and the memory will be cleaned up for us.<p>We also need to mutate some objects (for example class instance values with mutable members). So we need to turn to interior mutability here via the <code>RefCell&LTT></code> type - this allows us to expose a safe interface for mutating data even behind an immutable reference.<p>Introducing interior mutability unfortunately exposes us to a common problem in the form of <a href=https://doc.rust-lang.org/book/ch15-06-reference-cycles.html>leaked memory via circular references</a>. We are still in the land of “safe” rust, without dangling pointers and undefined behaviour. However the programme is now unsound. Consider the following, which will leak memory:<pre style=color:#dcdcdc;background-color:#1e1e1e><code><span>Class Person {}
</span><span>
</span><span>{
</span><span>  var alice = Person();
</span><span>  var bob = Person();
</span><span>  alice.friend = bob;
</span><span>  bob.friend = alice;
</span><span>}
</span><span>// alice and bob dropped, but still hold a reference to each other
</span></code></pre><p>This problem of circular references is often attempted to be solved via introduction of weak references - these look like <code>std::rc::Weak&LTT></code> and can be created via the <code>Rc::downgrade(&self)</code> method. If only weak references are left to an object, then it will be de-allocated and any attempt to access it will yield an empty <code>Option::None</code>. However this is often only viable if you can mould your domain to fit weak references; for example if you have a tree structure with weak references from child nodes to parent nodes (where parents will always outlive children). Alas, the Lox grammar allows for cyclic references, so we are stuck.<blockquote><p>Aside: one highlight of memory leaking I came across while building this project is the ability to <a href=https://doc.rust-lang.org/nomicon/leaking.html#rc>leak memory via reference count overflow</a>. Ouch.</blockquote><h2 id=can-it-be-done-safetm>Can It Be Done <code>safe™</code>?</h2><p>I spent a regretable amount of time reading up on the issue of garbage collection in Rust, and trying to make something work with pure Rust - I should have stopped earlier. Great amounts of research and review have gone into this topic, and an excellent review of the space can be found <a href=https://manishearth.github.io/blog/2021/04/05/a-tour-of-safe-tracing-gc-designs-in-rust/>here</a>. In particular, I was drawn to one particular implementation by Catherine West, who clearly faced the same issue when creating <a href=https://github.com/kyren/piccolo/tree/master>piccolo</a>, an interpreter for the Lua language (great blog post on the topic <a href=https://kyju.org/blog/rust-safe-garbage-collection/>here</a>).<p>I decided to run with this and use the underlying <a href=https://github.com/kyren/gc-arena>gc-arena</a> crate as a chance to learn from advanced Rust in the process. Internally I struggled with the notion that I wasn’t still leveraging “safe” Rust - the crate itself claims and looks to be safe, but under the hood uses unsafe. This is a blurred line you have to use your own judgement for; after all, the standard library is built on tons of unsafe Rust, even the <code>Rc&LTT></code> type I so wanted to leverage intitially. It’s all a trust game.<p>One interesting trick the <code>gc-arena</code> crate pulls to underpin its safety is leveraging lifetime subtyping and variance. The Rustonomicon covers the topic <a href=https://doc.rust-lang.org/nomicon/subtyping.html>in more depth</a>, but I’ll attempt to summarise here:<p>Covariance means that if <code>A</code> is a subtype of <code>B</code>, then <code>F&LTA></code> is a subtype of <code>F&LTB></code>. For example in Rust, immutable references are <em>covariant</em> over their lifetimes. What this means is if we have two immutable references <code>&'longer T</code> and <code>&'shorter T</code>, then since a longer lifetime outlives a shorter lifetime (so is a subtype), then the longer lifetime reference is a subtype of the shorter reference lifetime. This subtyping enables the compiler to implicitly shorten lifetimes to ease borrowing rules for covariant types. Types such as <code>&'a mut T</code> are <em>invariant</em> over their lifetimes and so require exact matching of type parameters, preventing both subtyping and supertyping relationships.<pre class=language-rust data-lang=rust style=color:#dcdcdc;background-color:#1e1e1e><code class=language-rust data-lang=rust><span style=color:#569cd6>fn </span><span>bar<</span><span style=color:#569cd6>'a</span><span>>() {
</span><span>    </span><span style=color:#569cd6>let</span><span> s: </span><span style=color:#569cd6>&'static str </span><span>= </span><span style=color:#d69d85>"hi"</span><span>;
</span><span>    </span><span style=color:#569cd6>let</span><span> t: </span><span style=color:#569cd6>&'a str </span><span>= s;  </span><span style=color:#608b4e>// this assignment is allowed since `s` is a subtype of `t`
</span><span>}
</span></code></pre><p>So to extend this idea, if we have some arena (of garbage collected values), we want to be sure that there are no live references to those values when garbage collection happens. By modelling pointer objects given out by the arena to be invariant over their lifetime, then we can isolate usage of these pointers and be confident that outside of these isolated areas, the pointers are safe to be collected! This is achieved using PhantomData, where can create invariance over the pointer’s lifetime with zero additional overhead, since PhantomData is a zero sized type.<pre class=language-rust data-lang=rust style=color:#dcdcdc;background-color:#1e1e1e><code class=language-rust data-lang=rust><span style=color:#608b4e>// Struct has the size of a machine pointer and can implement Copy - great!
</span><span style=color:#569cd6>pub struct </span><span>Gc<</span><span style=color:#569cd6>'gc</span><span>, T>
</span><span>    where T: ?Sized + 'gc {
</span><span>    </span><span style=color:#608b4e>// This type is a pointer.
</span><span>    ptr: NonNull&LTGcBox&LTT>>,
</span><span>
</span><span>    </span><span style=color:#608b4e>// We also contain a `PhantomData` type which marks the struct as being
</span><span>    </span><span style=color:#608b4e>// *invariant* over the 'gc lifetime since Cell&LTT> is invariant over T
</span><span>    _invariant: PhantomData&LTCell<</span><span style=color:#569cd6>&'gc </span><span>()>>,
</span><span>}
</span></code></pre><p>The core loop of our Lox interpreter jumps between two states: interpreting steps that mutate the arena state in isolated closures (which have their own invariant lifetime for handling pointers to heap values), and garbage collecting the arena.<h1 id=performance-optimisation>Performance Optimisation</h1><p>In optimising performance of my Lox implementation, I wanted to see how close I could get to the C <code>clox</code> implementation without dipping into unsafe Rust. Inheriting garbage collection through <code>gc-arena</code> already gives me a sort of “cheat” head-start here, through the use of pointer types that implement <code>Copy</code> and avoid the reference counting overhead of <code>Rc&LTT></code>. This final section outlines some of the techniques I learned about and used to improve runtime performance. I don’t go down a crazy rabbit hole, but thought it would be fun to pick up some tricks and document them here!<p>Overarching all of this is the use of a profiler to hone in on methods that were eating up CPU time. For this I am using MacOS and the <a href=https://github.com/mstange/samply/>samply</a> tool to output performance profiles and visualise using <a href=https://profiler.firefox.com/>FireFox Profiler</a>. I also used the native profiler “Apple Instruments” via <a href=https://github.com/cmyr/cargo-instruments/tree/master>cargo-instruments</a>, which additionally has tools to check for allocation metrics and monitor memory usage.<p>Using flamegraphs and flame charts here helped the most to notice areas of the code where performance gains could be found, and also encouraged me to further modularise the code to better understand which nested function calls were being hit heaviest. <a href=https://www.brendangregg.com/flamegraphs.html>Brendan Gregg</a> has great talks and resources on the topic for deeper understanding of how to analyse these charts.<p><img alt="Flame graph for a benchmark test run" src=/assets/content/crafting-interpreters/flame.png> <em>This chart for example shows that pushing values onto our Lox stack takes up ~3% of overall time in our interpreter steps. This could be a candidate for optimising e.g. through <a href=https://nnethercote.github.io/perf-book/bounds-checks.html>removing bounds checks using assertions</a></em><h2 id=profiling-tips-in-rust>Profiling Tips in Rust</h2><p>When profiling code, we are most interested in running <code>--release</code> builds of our code, which makes it faster at runtime through taking advantage of methods like clever compiling optimisations and spending time allocating variables to CPU registers. This is additionally at the cost of removing debug symbols, which are useful for debugging tools but bloat the executable.<p>However when profiling code, we need these debug symbols around to understand which functions have been sampled. How should we build in release mode with debug symbols, especially when we don’t want our final release build to contain them? Sounds fiddly to constantly amend the <code>Cargo.toml</code> file. Thankfully, here we can leverage cargo profiles and inheritance:<pre class=language-toml data-lang=toml style=color:#dcdcdc;background-color:#1e1e1e><code class=language-toml data-lang=toml><span>[</span><span style=color:gray>profile.release</span><span>]
</span><span style=color:#569cd6>codegen-units </span><span>= </span><span style=color:#b5cea8>1
</span><span style=color:#569cd6>lto </span><span>= </span><span style=color:#d69d85>"fat"
</span><span style=color:#569cd6>panic </span><span>= </span><span style=color:#d69d85>"abort"
</span><span>
</span><span>[</span><span style=color:gray>profile.profiling</span><span>]
</span><span style=color:#569cd6>inherits </span><span>= </span><span style=color:#d69d85>"release"
</span><span style=color:#569cd6>debug </span><span>= </span><span style=color:#569cd6>true
</span><span style=color:#569cd6>strip </span><span>= </span><span style=color:#569cd6>false
</span></code></pre><p>By running profiling using a dedicated cargo profile, we can inherit all the release properties of our regular release profile, while adding attributes required to do proper profile testing (<code>strip = false</code> here instructs the compiler to avoid removing debug symbols from the binary too). No more mental overhead remembering the state of your release profile.<p>In addition to profiles, I also wanted to mention “inlining”. The compiler will ultimately do what it wants - if a function is called many times (it’s hot) and is small / doesn’t have its own nested function calls, it is a good candidate for inlining, which will replace the function callsite with the function code itself. This reduces code jumping at runtime, at the cost of larger executable size. However when profiling an application, sometimes a function is inlined which disables you from analysing the line-by-line performance of the function at runtime. Here, subtle hinting to the compiler by using <code>#inline(never)</code> can be helpful.<h2 id=techniques>Techniques</h2><h3 id=hashing>Hashing</h3><p>This is mentioned by many other write-ups on Lox implementation, and switching the default <code>HashMap</code> hasher indeed did help with performance. However, more interestingly, larger speedups were found in my personal implementation through patching the underlying <code>Gc<'gc, T></code> pointer type provided by <code>gc-arena</code> (used as map keys).<p>Hashing and equality were implemented by dereferencing to the underlying type and using that. But if we look at <a href=https://doc.rust-lang.org/std/primitive.pointer.html#impl-Hash-for-*const+T>how primitive pointers implement <code>Hash</code> and <code>PartialEq</code></a>, it uses the pointer address itself. This simple change resulted in speedups of 30 to 50 percent across benchmarks.<pre class=language-rust data-lang=rust style=color:#dcdcdc;background-color:#1e1e1e><code class=language-rust data-lang=rust><span style=color:#569cd6>impl</span><span><</span><span style=color:#569cd6>'gc</span><span>, T: Hash </span><span style=color:#569cd6>+ </span><span style=color:#f33>?</span><span>Sized + </span><span style=color:#569cd6>'gc</span><span>> Hash for Gc<</span><span style=color:#569cd6>'gc</span><span>, T> {
</span><span>    </span><span style=color:#569cd6>fn </span><span>hash&LTH: Hasher>(</span><span style=color:#569cd6>&</span><span>self, state: </span><span style=color:#569cd6>&mut</span><span> H) {
</span><span>        ptr::hash(Gc::as_ptr(*self), state);
</span><span>    }
</span><span>}
</span></code></pre><h3 id=option-unwrapping>Option Unwrapping</h3><p>This one is less exciting, but caught me off guard. When fetching a variable from our local code block, a simple map from string to value is used. Lookup has potential to return an empty <code>Option::None</code>, and so <code>.ok_or(Error::UndefinedVariable(string))?</code> was used to safely unwrap the value or propagate a runtime error.<p>However, this method showed up a lot when profiling -it turns out that the construction of this error variant was not lazy, and so string cloning and allocation was happening even on the happy path. The way around this was to instead use <code>.ok_or_else(|| Error::UndefinedVariable(string))?</code> to defer the unhappy path allocations.<h3 id=enum-sizing>Enum Sizing</h3><p>One idea I had was to re-model the opcode enum that I used to represent operations in the virtual machine - initially I used data-carrying variants to model operations that required extra bytes of information to work. For example:<pre class=language-rust data-lang=rust style=color:#dcdcdc;background-color:#1e1e1e><code class=language-rust data-lang=rust><span style=color:#569cd6>enum </span><span>Operation {
</span><span>  Return,
</span><span>  Add,
</span><span>  Jump(</span><span style=color:#569cd6>u8</span><span>),  </span><span style=color:#608b4e>// u8 represents frame pointer jump distance
</span><span>  </span><span style=color:#569cd6>...
</span><span>}
</span></code></pre><p>This simple model worked well and is clean - when reading the <code>Jump</code> opcode, we immediately have access to jump offset data. However this enum would have a size in memory determined by the size of its largest variant (see details on memory and alignment <a href=https://garden.christophertee.dev/blogs/Memory-Alignment-and-Layout/Part-1#enums>here</a>). In general it is best to use smaller types where possible, to maximise the amount of data the CPU <a href=https://darkcoding.net/software/does-it-matter-what-type-i-use/>pulls into its cache lines</a> in one go, speeding up the programme runtime. So I switched up the enum:<pre class=language-rust data-lang=rust style=color:#dcdcdc;background-color:#1e1e1e><code class=language-rust data-lang=rust><span style=color:#569cd6>enum </span><span>Operation {
</span><span>  Return,
</span><span>  Add,
</span><span>  Jump,
</span><span>  </span><span style=color:#569cd6>...
</span><span>}
</span></code></pre><p>Now, when a <code>Jump</code> byte was read, an extra byte would need to be read to also fetch the jump offset data. In practice, this did not result in the sort of speed-ups I expected. Code needed to be stored as a <code>Vec&LTu8></code> rather than <code>Vec&LTOperation></code> to be able to also store additional opcode data, and conversions from <code>u8</code> to <code>Operation</code> were now showing up as hot in the profiler.<p>Going against my preference for type safety and leveraging enum-exhaustiveness, I decided to model operations instead as <code>const u8</code> values and match on those. This appeared to have a more noticeable impact on performance.<h2 id=results>Results</h2><p>Plotted below is a comparison of each model (baseline, baseline + optimisations, original book implementation <code>clox</code>) run on each benchmark test, using a weighted average across five runs:<p><img alt="Bar chart for benchmark results" src=/assets/content/crafting-interpreters/plot.png><p>I think overall this looks positive - using some simple profiling analysis, the performance of the safe Rust interpreter was cut down to within touching distance of <code>clox</code>. There is always more that can be investigated - further hotspots in the profiling, garbage collection parameter tuning, <a href=https://doc.rust-lang.org/rustc/profile-guided-optimization.html>profile-guided optimisation</a>, using <a href=https://rust-lang.github.io/packed_simd/perf-guide/introduction.html>SIMD instructions</a>, fine tuning hash and allocator combinations, … I’m happy enough with what I’ve learned to stop here!<p>My main takeaway from this exercise has been that safe Rust is powerful, but needs careful attention to really squeeze performance out of it (and likely also descension into unsafe Rust). Without a <code>clox</code> benchmark to compare against, how would I know that “my Rust is as fast as C”? My general feeling is that, in the hand-crafted cases, it often might not be. However, when the positive trade-off is better memory safety and protections that greatly improve developer velocity, I will take that trade nine times out of ten.</article><section class=pagination><a class="left arrow" href=https://d-j-harris.github.io/gradient-compression/>←</a><a title="Go to top" class=top id=goToTopBtn onclick=goToTop()>Top</a><script>var goToTop=(()=>{window.scrollTo({top:0,behavior:`smooth`})})</script><a class="right arrow" href=https://d-j-harris.github.io/chef-language/>→</a></section></main><footer><span class=copyright> © 2024 Daniel Harris. Powered by <a href=https://www.getzola.org><u>Zola</u></a> and <a href=https://github.com/semanticdata/mabuya>Mabuya</a>. </span><nav class=navigation-footer><ul><li><a href=https://github.com/D-J-Harris>GitHub</a><li><a href=https://www.linkedin.com/in/danj-harris/>Linkedin</a></ul></nav></footer>