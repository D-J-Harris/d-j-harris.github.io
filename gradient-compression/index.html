<!doctype html><html lang=en-US><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>Small Bytes</title><meta content=article property=og:type><meta content="Small Bytes" property=og:site_name><meta content="Small Bytes" property=og:title><meta content="Occasional posts about what I've been learning and doing" itemprop=about name=description><link href=https://d-j-harris.github.io/assets/icon.png rel=icon type=image/png><link href=https://d-j-harris.github.io/main.css rel=stylesheet><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css integrity=sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+ rel=stylesheet><script crossorigin defer integrity=sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js></script><script crossorigin defer integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk onload=renderMathInElement(document.body); src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js></script><body><header><section class=nav-container><a class=nav-title href=https://d-j-harris.github.io/> <img alt=Logo src=/assets/icon.png style=width:auto;height:3em> <h2>Small Bytes</h2> </a><nav><ul><li><a href=https://d-j-harris.github.io>Home</a><li><a href=https://d-j-harris.github.io/tags>Tags</a></ul></nav><div class=darkmode><input class=toggle id=darkmode-toggle tabindex=-1 type=checkbox><label for=darkmode-toggle id=toggle-label-light tabindex=-1><svg style="enable-background:new 0 0 35 35" viewbox="0 0 35 35" id=dayIcon version=1.1 x=0px xml:space=preserve xmlns=http://www.w3.org/2000/svg xmlns:xlink=http://www.w3.org/1999/xlink y=0px><title>Change to light mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5 S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5 C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6 C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9 c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44 l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5 c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06 L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2 C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29 c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7 C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5 c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"/></svg></label><label for=darkmode-toggle id=toggle-label-dark tabindex=-1><svg style="enable-background='new 0 0 100 100'" viewbox="0 0 100 100" id=nightIcon version=1.1 x=0px xml:space=preserve xmlns=http://www.w3.org/2000/svg xmlns:xlink=http://www.w3.org/1999/xlink y=0px><title>Change to dark mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571 C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23 c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369 c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65 c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"/></svg></label></div><script>const userPref=window.matchMedia(`(prefers-color-scheme: light)`).matches?`light`:`dark`;const currentTheme=localStorage.getItem(`theme`)??userPref;const syntaxTheme=document.querySelector(`#theme-link`);if(currentTheme){document.documentElement.setAttribute(`saved-theme`,currentTheme)};const switchTheme=a=>{let b=`saved-theme`,d=`theme`,e=`light`,c=`dark`;if(a.target.checked){document.documentElement.setAttribute(b,c);localStorage.setItem(d,c)}else{document.documentElement.setAttribute(b,e);localStorage.setItem(d,e)}};window.addEventListener(`DOMContentLoaded`,()=>{const a=document.querySelector(`#darkmode-toggle`);a.addEventListener(`change`,switchTheme,!1);if(currentTheme===`dark`){a.checked=!0}})</script></section></header><main><article class=post><section class=post-info><span>Written by</span> Daniel Harris<br><span>on </span><time datetime=2020-04-14>April 14, 2020</time></section><h1 class=post-title>Gradient Compression in Distributed Deep Learning</h1><section class=post-line></section><ul><li><a href=https://d-j-harris.github.io/gradient-compression/#introduction-to-deep-learning>Introduction to Deep Learning</a><li><a href=https://d-j-harris.github.io/gradient-compression/#scaling-up-model-training>Scaling Up Model Training</a> <ul><li><a href=https://d-j-harris.github.io/gradient-compression/#synchronisation>Synchronisation</a><li><a href=https://d-j-harris.github.io/gradient-compression/#parallelism>Parallelism</a></ul><li><a href=https://d-j-harris.github.io/gradient-compression/#how-costly-is-update-communication>How Costly is Update Communication?</a> <ul><li><a href=https://d-j-harris.github.io/gradient-compression/#gradient-quantisation>Gradient Quantisation</a><li><a href=https://d-j-harris.github.io/gradient-compression/#gradient-sparsification>Gradient Sparsification</a></ul><li><a href=https://d-j-harris.github.io/gradient-compression/#further-reading>Further Reading</a></ul><hr><h2 id=introduction-to-deep-learning>Introduction to Deep Learning</h2><p>Deep Learning (DL) has burst onto the scene as one of the most powerful tools we currently have for tackling problems in a range of domains, from machine translation and image captioning, to object and image detection, to speech recognition and much more. Making the distinction between deep learning and machine learning is an important one - much in the way that machine learning is one approach to investigating artificial intelligence, deep learning is a single implementation of machine learning. Particularly, DL focuses on artificial neural networks, abstractions of the function of the brain that aim to model data distributions through co-learning of layers of computational nodes, or neurons.<p>For a long time this concept wasn’t able to gain traction, since deep neural networks (DNNs) have many nuts and bolts to tune, and there just wasn’t the compute power or data available to do it with. However, along with advances in the two (especially practicality of GPU usage), DNNs began to achieve state of the art results in many fields. A landmark point was the emergence of convolutional neural networks (CNNs) in the field of image recognition in 2012 <a href=http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf>Krizhevsky et al.</a>. Results on the ImageNet task were still taking close to weeks of compute time, but there was a lot of promise.<p>Today, this hunger for more data and more computation only grows. For example in the field of natural language processing, transformer architectures are shown to have a correlation between performance and model complexity, resulting in an almost arms-race for unthinkably large models that ask for more and more from us… questions of research accessibility of these models and impact of large-scale training on the environment aside, let’s ask the question of how we can even begin to make this kind of problem scalable.<h2 id=scaling-up-model-training>Scaling Up Model Training</h2><p>Introducing: distributed deep learning. The idea is that in training a DNN, we should be able to save both time and resources by distributing the workload from one single machine, to multiple machines. Teamwork makes the dream work.<p>There are many important dimensions to talk about when it comes to distributed deep learning. I want to briefly cover the two main aspects here before pushing into the main topic of compression techniques, however please do check out these extensive surveys to delve deeper into further considerations, such as system architecture and gradient aggregation methods. (<a href=https://arxiv.org/pdf/1903.11314.pdf>Mayer et al.</a>, <a href=https://arxiv.org/pdf/1810.11787.pdf>Chahal et al.</a>, <a href=https://arxiv.org/pdf/2003.06307.pdf>Tang et al.</a>)<h3 id=synchronisation>Synchronisation</h3><p>In the standard setting, a single model is trained to learn a set of parameters \(\theta\) that minimise an objective function defined on the task at hand. In doing this, the model should be generalisable - it has learned a way to map data from a given distribution (e.g. the pixels in an image of cats), to the target result (e.g. is it a cat). This process is called optimisation, and a popular algorithm for this is Stochastic Gradient Descent (SGD). Mathematically this iterative method takes the form:<p>\[\mathbf{\theta}_{t+1} = \mathbf{\theta}_t - \eta \nabla_{\theta} J(\mathbf{x}_t; \theta)\]<p>for objective function \(J\), and sample of our data \(\boldsymbol{x}_t\) at timestep \(t\). The way this sample is taken already allows us to determine some of the speed with which the model trains: moving from the stochastic setting of one data point to a larger batch size of many data points can lead to computational speedup due to parallelism in GPUs. Perfect! However there is a tradeoff, where this can harm the generalisability of the model.<ul><li><p><em>Synchronous SGD</em>: these ‘gradient descent’ steps can be simply extended to a multi-worker setting. If batches of data are split up among multiple machines, then we can afford to use more at once. Each machine, or worker, takes a copy of the model and performs the update step above on its chunk of the data. These \(\nabla_{\theta}\) updates are then aggregated to a central model to complete the process. One key problem with this is that there can be single failure points if any one worker is too slow.</p><li><p><em>Asynchronous SGD</em>: Workers could instead send their step updates as and when they have them, eliminating this straggler problem and increasing parallelisation. However in practice this technique is not preferred, as it can lead to instability in model convergence, due to some workers maybe training on ‘stale’ models.</p></ul><h3 id=parallelism>Parallelism</h3><ul><li><p><em>Data Parallelism</em>: In this setting, the data that we want to train on is split into non-overlapping batches, and distributed to every worker which has identical copies of the model. Here, because each worker is sharing parameters of the model, then they have to communicate their updates (aka what they’ve learned about the parameters from their data chunk) regularly. This lends data parallelism being a better scheme for compute-intensive, but low-parameter-number models such as CNNs.</p><li><p><em>Model Parallelism</em>: In the case where a model is too big to fit in memory on a single device, it is useful to rather split the model into layered chunks to train separately. This splitting is often done using reinforcement learning. Due to the feed-forward nature of DNNs, this method requires heavy communication of updates and potential bottlenecks, making data parallelism more common practice.</p></ul><h2 id=how-costly-is-update-communication>How Costly is Update Communication?</h2><p>Pretty costly. For example, the model size of ResNet-50 is over 110MB - calculating gradients for making gradient descent updates contain millions of floating point values, and so the size of these scale proportionally. Considering a standard Ethernet connection can have a speed of around 1Gbps, then this makes scaling up these distributed systems a problem, as communication between workers can prove to be a bottleneck or rate limiting factor in training models at scale, thus under-utilising computing resources.<p>There are three main methods in tackling reducing communication overhead in distributed deep learning. These are:<ul><li><p>Reducing Model Precision - by changing the size of the model parameters, for example from a common 32-bit floating point representation to 16-bit, then exchanged gradients are inherently smaller in memory and so cost less to communicate.</p><li><p>Improving Communication Scheduling - imagine you want to print something, but everybody else hits print at the same time, and now you have to wait much longer to get your result back. By being smart about which times gradients should be sent, distributed systems can avoid exceeding bandwidth limits and therefore not bottleneck the process.</p><li><p>Compressing Gradients - this is where my work will step in. This area looks at the ability to compress gradients to smaller sizes before they are communicated, in such a way that enough information is retained to have minimal impact on the accuracy of the resulting model. Due to compression limitation these techniques are often lossy, and fall under mainly two broad categories: gradient quantisation, and gradient sparsification (Figure 1).</p></ul><h3 id=gradient-quantisation>Gradient Quantisation</h3><p>The idea behind gradient quantisation is to reduce the bit-representation of gradients to make them less costly to communicate. There are three popular approaches to quantisation:<p><a href=https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf>Seide et al.</a> demonstrate that gradients can be compressed to just 1-bit (a representation of their sign) and communicated to provide speedups of up to 100 times on a large 160 million parameter model. As with many techniques in the field, this uses an error-feedback scheme, where quantisation errors are accumulated and added to the next batch, which can achieve the same rate of convergence as SGD (<a href=https://arxiv.org/pdf/1901.09847.pdf>Karimireddy et al.</a>).<p><a href=https://arxiv.org/pdf/1705.07878.pdf>Wen et al.</a> introduce TernGrad, a scheme that works by quantising on only three numerical levels {-1,0,1}. Unlike 1-bit quantisation, this is applicable also to gradients arising from convolutional layers. The authors use two techniques to be able to additionally prove the convergence of a model under this quantisation, under the assumption of a bound on the gradients.<ul><li>Firstly,when ternarising gradients a scaler function is applied that scales the operation relative to the largest absolute value in the gradient. This ternarising is therefore done on a layer-by-layer basis, to avoid scaling by global absolute values, and therefore provide a stronger bound on the gradients.<li>Secondly, gradient clipping is used to address the issue of dynamic ranges in the bound gap.</ul><p><a href=https://arxiv.org/pdf/1610.02132.pdf>Alistarh et al.</a> introduce Quantised SGD (QSGD), which is a family of algorithms that are able to give a tradeoff between bits communicated and variance of the compressed gradients. This allows for tradeoff between bandwidth communication and model convergence time. In words, what QSGD does is takes a probabilistic approach on quantisation, using an encoding scheme that emphasises the likelihood of gradient values into its compression. Compared to TernGrad, this method has no hyperparameters and guarantees convergence under standard assumptions only. However, <a href=https://arxiv.org/pdf/1705.07878.pdf>Wen et al.</a> point out that eliminating accuracy loss in QSGD requires at least 16 levels (4-bit) of encoding, whereas this is achieved with only 3 levels using TernGrad.<h3 id=gradient-sparsification>Gradient Sparsification</h3><p>The other family of compression techniques is called gradient sparsification, which involves dropping any gradient values which transmit little to no information in parameter updates. Note that the compression rate of previously mentioned quantisation methods is at best 32 times, due to the limitation in how many bits can be reduced from the standard 32-bit gradients. Therefore compression rates can be greatly helped by sparsification.<p><a href=https://arxiv.org/pdf/1704.05021.pdf>Aji et al.</a> note that gradient updates are positively skewed, as most updates are near zero. This motivates the use of cheaper communication via sparse matrix transmission, which is done by mapping a large fraction of updates to zero. This fraction threshold is expensive to find, however can be approximated as 99% through sampling.<p><a href=https://www.isca-speech.org/archive/interspeech_2015/papers/i15_1488.pdf>Strom et al.</a> adopt a similar method, however choose a static threshold value above which gradients should be zero. This resulted in high speed gains of 54 times for only a 1.8% error on a particular speech recognition task, however it should be noted that the threshold was a hyperparameter that was difficult to tune. <a href=https://dl.acm.org/doi/10.5555/3018874.3018875>Dryden et al.</a> on the other hand employ an adaptive threshold technique, which determines a threshold of gradients to communicate every iteration that results in a fixed compression ratio throughout training. This involves extra overhead compute time in sorting gradient vectors, which is addressed in more recent works.<h2 id=further-reading>Further Reading</h2><p>This quick tour of distributed deep learning reflects on the richness in this area for future study. There are a host of more recent exciting advances, from Sketch communication <a href=https://arxiv.org/pdf/1903.04488.pdf>Ivkin et al.</a>, to low-rank approximation gradient compression <a href=https://arxiv.org/pdf/1905.13727.pdf>Vogels et al.</a> and more. I’ve even left out of my description the current most used gradient compression technique, Deep Gradient Compression (DGC) <a href=https://arxiv.org/pdf/1712.01887.pdf>Lin et al.</a>. This technique enjoys impressive whole-model gradient compression ratios of up to 600 times(!) using momentum correction and masking to address issues with staleness in error accumulation and feedback.</article><section class=pagination><a title="Go to top" class=top id=goToTopBtn onclick=goToTop()>Top</a><script>var goToTop=(()=>{window.scrollTo({top:0,behavior:`smooth`})})</script><a class="right arrow" href=https://d-j-harris.github.io/crafting-interpreters/>→</a></section></main><footer><span class=copyright> © 2024 Daniel Harris. Powered by <a href=https://www.getzola.org><u>Zola</u></a> and <a href=https://github.com/semanticdata/mabuya>Mabuya</a>. </span><nav class=navigation-footer><ul><li><a href=https://github.com/D-J-Harris>GitHub</a><li><a href=https://www.linkedin.com/in/danj-harris/>Linkedin</a></ul></nav></footer>